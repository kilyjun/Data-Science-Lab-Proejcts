{"cells":[{"cell_type":"markdown","id":"54aae887-dd06-4b82-8a6b-8c211bf14023","metadata":{"id":"54aae887-dd06-4b82-8a6b-8c211bf14023"},"source":["# Week 11: Optical Flow\n","\n","<font size=\"6\"> Laboratory 8 </font> <br>\n","<font size=\"3\"> Last updated August 17, 2022 </font>"]},{"cell_type":"markdown","id":"8119d849-3a32-40a5-839b-4304b5aba778","metadata":{"id":"8119d849-3a32-40a5-839b-4304b5aba778"},"source":["## <span style=\"color:orange;\"> 00. Content </span>\n","\n","<font size=\"5\"> Mathematics </font>\n","- Taylor series\n","- Vector fields\n","- Polar coordinates\n","    \n","<font size=\"5\"> Programming Skills </font>\n","- Vectorization\n","    \n","<font size=\"5\"> Embedded Systems </font>\n","- Thonny and MicroPython\n","\n","## <span style=\"color:orange;\"> 0. Required Hardware </span>\n","- Microcontroller: Raspberry Pi Pico\n","- Breadboard\n","- USB connector\n","- Camera"]},{"cell_type":"markdown","id":"e8d0c2d7-5aa4-45c7-969e-f90b25eb827c","metadata":{"id":"e8d0c2d7-5aa4-45c7-969e-f90b25eb827c"},"source":["<h3 style=\"background-color:lightblue\"> Write your name and email below: </h3>\n","\n","**Name:** me \n","\n","**Email:** me @purdue.edu"]},{"cell_type":"code","execution_count":null,"id":"dad061d2-2e0e-4425-975f-84ac0a2709e6","metadata":{"id":"dad061d2-2e0e-4425-975f-84ac0a2709e6"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import cv2"]},{"cell_type":"markdown","id":"3bee96ba-87da-4dc9-9c13-7697f07b701c","metadata":{"id":"3bee96ba-87da-4dc9-9c13-7697f07b701c"},"source":["## <span style=\"color:orange;\"> 1. Optical Flow and Motion Tracking </span>\n","\n","Optical flow is the motion and light patterns observed when an object and camera move relative to each other. The velocity of objects in a video can be estimated using optical flow. This is especially relevant in computer vision tasks when the goal is motion-based object detection or robot navigation."]},{"cell_type":"markdown","id":"8a3280ee-5b14-4203-b8c5-93f2b06b35d6","metadata":{"id":"8a3280ee-5b14-4203-b8c5-93f2b06b35d6"},"source":["Let $f(x,y,t)$ be a continuous grayscale video mapping $\\mathbb{R}^3$ to $\\mathbb{R}$.\n","The variables $x$ and $y$ represent pixel coordinates and $t$ is the frame number.\n","The output is the pixel intensity, a integer between $0$ and $255$, at location $(x,y)$ in frame $t$.\n","When we read in an image in Python we are observing a sampling of points of $f(x,y,t)$ which we will call $f_d(n,m,t)$.\n","So, $\n","    f_d(n,m,t) = f(n\\Delta i, m \\Delta j,t),$ \n","where $\\Delta i$ and $\\Delta j$ are the sampling distances along the x and y-axis, respectively."]},{"cell_type":"markdown","id":"db04d1dd-6413-4ef6-bd0e-d73f64e81b8d","metadata":{"id":"db04d1dd-6413-4ef6-bd0e-d73f64e81b8d"},"source":["Let's assume there is a object at the location $(x,y,t)$ and after a small $\\Delta t$ step forward in time, the object moves from $(x,y)$ to $(x+\\Delta x,y+\\Delta y)$ in the video.\n","Using Taylor series to expand $f(x,y,t)$ and truncating after the first degree terms, we can estimate the velocity of the object.\n","\\begin{align*}\n","f(x+\\Delta x,y+\\Delta y,t+\\Delta t) \\approx f(x,y,t) +\n","    \\frac{\\partial f}{dx}\\Delta x + \n","    \\frac{\\partial f}{dy}\\Delta y + \n","    \\frac{\\partial f}{dt}\\Delta t \n","\\end{align*}\n","\n","Assuming the object's brightness does not change as it moves, $f(x+\\Delta x,y+\\Delta y,t+\\Delta t)=f(x,y,t)$, so \n","\\begin{align*}\n","    \\frac{\\partial f}{dx}\\Delta x + \n","    \\frac{\\partial f}{dy}\\Delta y + \n","    \\frac{\\partial f}{dt}\\Delta t = 0\n","\\end{align*}\n","\n","Divide both sides by $\\Delta t$ to get\n","\\begin{align*}\n","    \\frac{\\partial f}{dx} \\frac{\\Delta x}{\\Delta t} + \n","    \\frac{\\partial f}{dy} \\frac{\\Delta y}{\\Delta t} + \n","    \\frac{\\partial f}{dt} = 0 \n","\\end{align*}\n","\n","Notice that $\\frac{\\partial f}{dx}$ and $\\frac{\\partial f}{dy}$ are the image gradients we calculated in the lab on edge detection, and $\\frac{\\partial f}{dt}$ is the pixel-wise difference between frame $t+\\Delta t$ and frame $t$.\n","That means in the last equation the velocity components $\\frac{\\Delta x}{\\Delta t}$ and $\\frac{\\Delta y}{\\Delta t}$ are unknown and the other 3 terms $\\frac{\\partial f}{dx},\\frac{\\partial f}{dy},$ and  $\\frac{\\partial f}{dt}$ we can calculate, but we can't solve for 2 unknowns with only one equation. We're going to need some more assumptions."]},{"cell_type":"markdown","id":"55a9c661-357d-4629-a437-4fdbc0b30898","metadata":{"id":"55a9c661-357d-4629-a437-4fdbc0b30898"},"source":["### <span style=\"color:red\"> Exercise 1 </span>\n","\n","Look up \"the aperature problem\" as it relates to optical flow. In a few sentences, describe what it is and how it impacts the type of objects we are able to track with optical flow. \n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 1 Below </h3>"]},{"cell_type":"markdown","id":"ac7935d3-e1ba-48ea-aecf-c273c039d6f3","metadata":{"id":"ac7935d3-e1ba-48ea-aecf-c273c039d6f3"},"source":["The aperture problem occurs when it's difficult to determine an object's motion in a scene due to limited visibility or uniform textures. This issue affects optical flow because it relies on local measurements to track motion. "]},{"cell_type":"markdown","id":"6f43726e-544a-4278-840c-410a915f4b14","metadata":{"id":"6f43726e-544a-4278-840c-410a915f4b14"},"source":["### Farneback Optimal Flow\n","\n","The assumption that $f(x+\\Delta x,y+\\Delta y,t+\\Delta t)=f(x,y,t)$ is reasonable for most situations. However, if there is a lot of change from frame to frame, for example a helicopter-mounted camera undergoing strong vibrations, then our results will not be as good. In 2003, Gunnar Farneback proposed a solution to this problem. To fully explain his method, a good knowledge of linear algebra and least squares minimization is necessary, but we will give a general overview.\n","\n","The idea behind Farneback's algorithm for calculating optimal flow is:\n","- Starting with the previous frame, reduce the resolution of the image.\n","- Estimate the values of a small patch of pixels with a quadratic polynomial (like how we did polynomial interpolation in 1 dimension but this is in 2 dimensions).\n","- In the next frame, do the same quadratic estimation.\n","- With the 2 simplified quadratic functions, it's \"easy\" to calculate the displacement of the patch of pixels (this is where the linear algebra and least squares minimization comes into play).\n","- Increase the resolution of the frame and repeat the process again. \n","\n","By repeating on higher and higher resolution images, we are able to capture both big and small displacements. After refining how much displacement there is between the two frames, the result is $\\frac{\\Delta x}{\\Delta t}$ and $\\frac{\\Delta y}{\\Delta t}$ for each pixel in the image, so we call this type of process dense optimal flow. "]},{"cell_type":"markdown","id":"ddfda3b1-bce7-4b63-aaba-d2c78b7304e2","metadata":{"id":"ddfda3b1-bce7-4b63-aaba-d2c78b7304e2"},"source":["### <span style=\"color:red\"> Exercise 2 </span>\n","\n","Read through the following cell. In a few sentences, describe what the code does.\n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 2 Below </h3>"]},{"cell_type":"markdown","id":"536ea710","metadata":{"id":"536ea710"},"source":["This code reads a video file, resizes the frames to 25% of their original size and converts to grayscale. Then, in a loop, it readsnconsecutive frames, calculates the optical flow for each pixel, and shows the next frame, until the end of the video or when 'Q' is pressed."]},{"cell_type":"code","execution_count":null,"id":"584e9dc8-1814-434e-bd26-1b63d33bd759","metadata":{"id":"584e9dc8-1814-434e-bd26-1b63d33bd759"},"outputs":[],"source":["vid = cv2.VideoCapture('test_vid.mov')\n","height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","width  = vid.get(cv2.CAP_PROP_FRAME_WIDTH) \n","scale = 0.25\n","new_size = (int(width*scale),int(height*scale))\n","\n","success, frame1 = vid.read()\n","frame1 = cv2.resize(frame1,dsize=new_size)\n","previous = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n","\n","while True:\n","    success, frame2 = vid.read()\n","    if not success:\n","        print(\"Unable to read frame. Exiting ...\")\n","        break\n","    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n","    next = cv2.resize(next,dsize=new_size)\n","    \n","    # flow is a numpy 2 dimensional array and \n","    # flow[i,j] = [dx/dt,dy/dt] for pixel (i,j)\n","    flow = cv2.calcOpticalFlowFarneback(previous,next, flow = None, \n","                    # don't worry about these parameters for now\n","                    pyr_scale = 0.5, \n","                    levels = 3, \n","                    winsize = 10, \n","                    iterations = 3, \n","                    poly_n = 5, \n","                    poly_sigma = 1.1, flags = 0)\n","    cv2.imshow('next_frame',next)\n","    k = cv2.waitKey(30) & 0xff\n","    if k == ord('q'): # press Q on keyboard to stop\n","        break\n","    previous = next\n","\n","vid.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"90364543-da98-4ddb-af78-30b24a3a9454","metadata":{"id":"90364543-da98-4ddb-af78-30b24a3a9454"},"source":["At each step `flow` defines a vector field. At pixel location $[i,j]$, we define a vector $\\langle dx/dt, dy/dt\\rangle$. This vector has a magnitude and a direction. "]},{"cell_type":"markdown","id":"8d8051a4-d81e-442d-b33b-a5fa3f399cd6","metadata":{"id":"8d8051a4-d81e-442d-b33b-a5fa3f399cd6"},"source":["### <span style=\"color:red\"> Exercise 3 </span>\n","\n","1. What are the formulas to calculate the magnitude and direction of a vector $\\langle x,y \\rangle$?\n","2. Why would computing the magnitude and direction of $\\langle x,y \\rangle$ be the same as converting the Cartesian ordered pair $(x,y)$ to polar coordinates?\n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 3 Below </h3>"]},{"cell_type":"markdown","id":"01774e48-9554-4217-95a1-30dde2023207","metadata":{"id":"01774e48-9554-4217-95a1-30dde2023207"},"source":["Magnitude: sqrt(ùë•^2 + ùë¶^2)\n","Direction: tanŒ∏ = y / x (in radians)\n","\n","The magnitude represents the distance from the origin to the point (ùë•,ùë¶), and the direction represents the angle between the positive x-axis and the line connecting the origin to the same point, which is exactly what polar coordinates describe."]},{"cell_type":"markdown","id":"f745c077-5743-4689-a093-6d13389fe9f3","metadata":{"id":"f745c077-5743-4689-a093-6d13389fe9f3"},"source":["### <span style=\"color:red\"> Exercise 4 </span>\n","\n","Pick two frames to calculate `flow`. Use the matplotlib function [quiver](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.quiver.html) to plot the vector field using arrows. \n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 4 Below </h3>"]},{"cell_type":"code","execution_count":null,"id":"9f613062-8d15-48b4-a553-ff88b714bf62","metadata":{"id":"9f613062-8d15-48b4-a553-ff88b714bf62"},"outputs":[],"source":["# Read two frames from the video\n","vid = cv2.VideoCapture('test_vid.mov')\n","_, frame1 = vid.read()\n","_, frame2 = vid.read()\n","vid.release()\n","\n","# Convert frames to grayscale\n","previous = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n","next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n","\n","# Calculate optical flow using the Farneback method\n","flow = cv2.calcOpticalFlowFarneback(previous, next, flow=None, pyr_scale=0.5, levels=3, winsize=15, iterations=3, poly_n=5, poly_sigma=1.1, flags=0)\n","\n","# Plot the vector field using arrows with the quiver function\n","x, y = np.meshgrid(np.linspace(start=0, stop=flow.shape[1], num=flow.shape[1], endpoint=False), np.linspace(start=0, stop=flow.shape[0], num=flow.shape[0], endpoint=False))\n","dxdt = flow[..., 0]\n","dydt = flow[..., 1]\n","\n","plt.quiver(x, y, dxdt, dydt, color='b', units='xy', scale=1)\n","plt.gca().invert_yaxis()\n","plt.show()\n","\n","\n","# example for a plotting a constant vector field where each vector is <1,1>\n","x,y = np.meshgrid(np.linspace(start=0, stop=10, num=10, endpoint=False), \n","                   np.linspace(start=0, stop=10, num=10 , endpoint=False))\n","dxdt = 1\n","dydt = 1\n","plt.quiver(x,y,dxdt,dydt,color='b', units='xy', scale=1)\n","plt.show()"]},{"cell_type":"markdown","id":"bbb052f4-af68-400b-b53c-b1267fa2d898","metadata":{"id":"bbb052f4-af68-400b-b53c-b1267fa2d898"},"source":["Another way to visualize the vector field is using color to represent angles and the brightness of the color to represent the magnitude. The HSV or (Hue Saturation Value) color space is the perfect candidate. We are going to relate Hue to the angle of the vector. The [image](https://github.com/TheDataScienceLabs/DataLab_Multivariate_Calculus/blob/main/book/labs/2_Video_Labs/hue_angle.jpg) below shows each hue at full saturation for different angles.\n","\n","![img](hue_angle.jpg)"]},{"cell_type":"markdown","id":"921795a7-4e0e-412c-a868-c7bfa300f39c","metadata":{"id":"921795a7-4e0e-412c-a868-c7bfa300f39c"},"source":["### <span style=\"color:red\"> Exercise 5 </span>\n","\n","Fill in the code below to compute the magnitude and direction of the velocity vectors at each pixel location. Now we can visualize how the vector field changes from frame to frame and in what direction the objects in the frame are moving. \n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 5 Below </h3>"]},{"cell_type":"code","execution_count":null,"id":"e529794a-42aa-4c4d-9abf-78d8a6e86ab9","metadata":{"id":"e529794a-42aa-4c4d-9abf-78d8a6e86ab9"},"outputs":[],"source":["cap = cv2.VideoCapture('test_vid.mov')\n","height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","width  = cap.get(cv2.CAP_PROP_FRAME_WIDTH) \n","scale = 0.25\n","\n","ret, frame1 = cap.read()\n","frame1 = cv2.resize(frame1,dsize=(int(width*scale),int(height*scale)))\n","previous = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n","hsv = np.zeros_like(frame1)\n","hsv[...,1] = 255 # always at full saturation\n","\n","while True:\n","    ret, frame2 = cap.read()\n","    if not ret:\n","        print(\"Unable to read frame. Exiting ...\")\n","        break\n","    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n","    next = cv2.resize(next,dsize=(int(width*scale),int(height*scale)))\n","    flow = cv2.calcOpticalFlowFarneback(previous,next, flow = None, \n","                    pyr_scale = 0.5, \n","                    levels = 3, \n","                    winsize = 10, \n","                    iterations = 3, \n","                    poly_n = 5, \n","                    poly_sigma = 1.1, flags = 0)\n","\n","    # compute magnitute and angle for each vector in flow\n","    # mag and ang should have the same dimensions as the variable next \n","    mag = # fill in\n","    ang = # fill in\n","    # ang needs to be between 0 and 2pi\n","\n","    hsv[...,0] = ang*180/np.pi / 2 # convert radians to degrees then divide by 2\n","    hsv[...,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n","    rgb = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n","\n","    cv2.imshow('frame2',rgb)\n","    k = cv2.waitKey(30) & 0xff\n","    if k == ord('q'): # press Q on keyboard to stop\n","        break\n","    previous = next\n","\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","id":"efb70cbd-670e-4745-be83-8822a81060b4","metadata":{"id":"efb70cbd-670e-4745-be83-8822a81060b4"},"source":["## <span style=\"color:orange;\"> 2. Repeat with Your Own Video </span>\n","\n","As you did in the previous lab, you will be repeating some of exercises again with your own recorded video. When recording your video, take into account what is being done in the lab and consider if the video you have will produce good results."]},{"cell_type":"markdown","id":"2ea466e0-1818-45f5-ae35-8905a8411e29","metadata":{"id":"2ea466e0-1818-45f5-ae35-8905a8411e29"},"source":["### <span style=\"color:red\"> Exercise 6 </span>\n","\n","Save a few seconds worth of frames from the video feed. There needs to be at least one moving object. Pick two frames to calculate flow and plot the associated vector field.\n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 6 Below </h3>"]},{"cell_type":"code","execution_count":null,"id":"fede61be-4db7-4b12-a850-a6957db54663","metadata":{"id":"fede61be-4db7-4b12-a850-a6957db54663"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"fc975740-e432-4996-878c-ddddfe350452","metadata":{"id":"fc975740-e432-4996-878c-ddddfe350452"},"source":["### <span style=\"color:red\"> Exercise 7 </span>\n","\n","Use the completed code from Exercise 5 to show the optical flow of your own video. \n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for Exercise 7 Below </h3>"]},{"cell_type":"code","execution_count":null,"id":"79cd2da8-7208-44b9-9a01-13886f6a1a0b","metadata":{"id":"79cd2da8-7208-44b9-9a01-13886f6a1a0b"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"20db75fa-c06a-4247-9511-ec1e48ff2969","metadata":{"id":"20db75fa-c06a-4247-9511-ec1e48ff2969"},"source":["## <span style=\"color:green\"> Reflection </span>\n","\n","__1. What parts of the lab, if any, do you feel you did well? <br>\n","2. What are some things you learned today? <br>\n","3. Are there any topics that could use more clarification? <br>\n","4. Do you have any suggestions on parts of the lab to improve?__\n","\n","<h3 style=\"background-color:lightblue\"> Write Answers for the Reflection Below </h3>"]},{"cell_type":"markdown","id":"8859ccbb-7da1-4ac3-9a51-f092290e48f1","metadata":{"id":"8859ccbb-7da1-4ac3-9a51-f092290e48f1"},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"5733b5ebf5ecec2b002a59c36710d44decb4334b28aff8074b4cca610e6649ad"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}